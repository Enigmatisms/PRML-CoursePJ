## 数据的意义

---

- `matrix_train.mtx` （或者test）是稀疏矩阵中为1的值（所在的xy位置），其中第一列开头写了40000：
  - 40000对应了40000行，2000对应了2000列
  - 也就是说，此数据是按照列来遍历的：首先第一列为1的行的id，依次类推。个人认为，需要写一个make dataset，将数据离散化（可以存成numpy binary，由于应该没有除了1以外的值，所以可以存成u8_char），然后需要写一个make custom dataset

### TODO

​		train / test labels 需要提前转化为numpy 数组（u8），在之后的数据集加载中直接load每一个数组。需要这样的转化函数（这步很简单）

---

- 注意：data是ATCG序列，需要做encoding，ATCG的encoding怎么做？这里可以单独写一个encoding模块，将encoded ATCG单独保存为数据集（data），对应的2000维向量变成label：
  - encoding尽量满足正交性原理：不同的维度之间尽可能正交：正交可以保证encoding出来的四个特征不会重合：可以使用长为12的encoding（$[1, 0, 0, 0], [\sqrt{0.5}, \sqrt{0.5}0, 0], [0.5, -0.5, 0.5, -0.5]$）这样形成encoding，最长的也就18000，对应于一个$135\times 135$的图像。
  - 所以需要首先将所有ATCG序列转化为encoding并保存为numpy binary（便于取出）

### TODO

​		ATCG先行转化为encoding（可以试着转化为1* 12的float数组），并且将所有序列padding至长度为1500，并使添加masking，获得$N \times (C+1)$的数据。为了使得数据集加载快速并且耗费尽可能少的内存，需要自己写一个Dataset类并使用Pytorch的Dataloader。

---

- 关于网络架构，是否需要masking？
  - 有两种解决方案：（1）masking，这种方案处理起来也不难，假设原始的输入序列是：$N\times C$的，那么我们在最后一维concat上1维度，使得最后的model输入总是$N\times (C+1)$，其中N是最大sequence长度，则：
    - 最后输入：1500 * 13: 约为 140 * 140单通道图像，计算量应该也不会特别离谱。则接下来的处理是：由于我们不能直接segment into patches（粒度很可能不够，也就是说，为了保证transformer输入的patch较少（以免产生过大的计算开销）），

### TODO

​		可以使用SwinTransformer。个人的想法是：

- 输入的序列首先通过1D卷积（3层，kernel大小为3，那么最后形成的感受野为7），对于大小为10的1Dpatch，相当于最后的感受野为16。
- 使用SwinTransformer的方法：以免出现长距离依赖的问题。patch大小为10（通过MSA - shift MSA）修改encoding（/embedding）
  - 1500 -> 750 (merging) 对应感受野变为32
  - 750 -> 375 (merging) 对应感受野变为64
  - 375 -> pad 1 (376) -> merging (188) pad 2 -> (190 保证patch大小为10) 对应感受野变为128
  - 190 -> merging 95 对应感受野变为256 已经够用
  - 最后输出怎么做？做一个average pool，之后进入classification head即可
